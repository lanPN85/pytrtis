# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: grpc_service_v2.proto

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from pytrtis.gen import model_config_pb2 as model__config__pb2


DESCRIPTOR = _descriptor.FileDescriptor(
  name='grpc_service_v2.proto',
  package='nvidia.inferenceserver',
  syntax='proto3',
  serialized_options=None,
  serialized_pb=b'\n\x15grpc_service_v2.proto\x12\x16nvidia.inferenceserver\x1a\x12model_config.proto\"\x13\n\x11ServerLiveRequest\"\"\n\x12ServerLiveResponse\x12\x0c\n\x04live\x18\x01 \x01(\x08\"\x14\n\x12ServerReadyRequest\"$\n\x13ServerReadyResponse\x12\r\n\x05ready\x18\x01 \x01(\x08\"2\n\x11ModelReadyRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\x03\"#\n\x12ModelReadyResponse\x12\r\n\x05ready\x18\x01 \x01(\x08\"\x17\n\x15ServerMetadataRequest\"K\n\x16ServerMetadataResponse\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\t\x12\x12\n\nextensions\x18\x03 \x03(\t\"5\n\x14ModelMetadataRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\x03\"\xa7\x02\n\x15ModelMetadataResponse\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08versions\x18\x02 \x03(\x03\x12\x10\n\x08platform\x18\x03 \x01(\t\x12L\n\x06inputs\x18\x04 \x03(\x0b\x32<.nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata\x12M\n\x07outputs\x18\x05 \x03(\x0b\x32<.nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata\x1a?\n\x0eTensorMetadata\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08\x64\x61tatype\x18\x02 \x01(\t\x12\r\n\x05shape\x18\x03 \x03(\x03\"\x9b\x02\n\x0eInferParameter\x12\x14\n\nbool_param\x18\x01 \x01(\x08H\x00\x12\x15\n\x0bint64_param\x18\x02 \x01(\x03H\x00\x12\x16\n\x0cstring_param\x18\x03 \x01(\x0cH\x00\x12V\n\x15response_detail_param\x18\x04 \x01(\x0b\x32\x35.nvidia.inferenceserver.InferParameter.ResponseDetailH\x00\x1aX\n\x0eResponseDetail\x12\r\n\x05model\x18\x01 \x01(\x08\x12\x0f\n\x07request\x18\x02 \x01(\x08\x12\x12\n\nstatistics\x18\x03 \x01(\x08\x12\x12\n\nparameters\x18\x04 \x01(\x08\x42\x12\n\x10parameter_choice\".\n\x11StatisticDuration\x12\r\n\x05\x63ount\x18\x01 \x01(\x04\x12\n\n\x02ns\x18\x02 \x01(\x04\"\x87\x03\n\x0fInferStatistics\x12:\n\x07success\x18\x01 \x01(\x0b\x32).nvidia.inferenceserver.StatisticDuration\x12\x37\n\x04\x66\x61il\x18\x02 \x01(\x0b\x32).nvidia.inferenceserver.StatisticDuration\x12\x38\n\x05queue\x18\x03 \x01(\x0b\x32).nvidia.inferenceserver.StatisticDuration\x12@\n\rcompute_input\x18\x04 \x01(\x0b\x32).nvidia.inferenceserver.StatisticDuration\x12@\n\rcompute_infer\x18\x05 \x01(\x0b\x32).nvidia.inferenceserver.StatisticDuration\x12\x41\n\x0e\x63ompute_output\x18\x06 \x01(\x0b\x32).nvidia.inferenceserver.StatisticDuration\"\xe7\x01\n\x13InferTensorContents\x12\x14\n\x0craw_contents\x18\x01 \x01(\x0c\x12\x15\n\rbool_contents\x18\x02 \x03(\x08\x12\x14\n\x0cint_contents\x18\x03 \x03(\x05\x12\x16\n\x0eint64_contents\x18\x04 \x03(\x03\x12\x15\n\ruint_contents\x18\x05 \x03(\r\x12\x17\n\x0fuint64_contents\x18\x06 \x03(\x04\x12\x15\n\rfp32_contents\x18\x07 \x03(\x02\x12\x15\n\rfp64_contents\x18\x08 \x03(\x01\x12\x17\n\x0fstring_contents\x18\t \x03(\x0c\"\xf1\x07\n\x11ModelInferRequest\x12\x12\n\nmodel_name\x18\x01 \x01(\t\x12\x15\n\rmodel_version\x18\x02 \x01(\x03\x12\n\n\x02id\x18\x03 \x01(\t\x12M\n\nparameters\x18\x04 \x03(\x0b\x32\x39.nvidia.inferenceserver.ModelInferRequest.ParametersEntry\x12J\n\x06inputs\x18\x05 \x03(\x0b\x32:.nvidia.inferenceserver.ModelInferRequest.InferInputTensor\x12U\n\x07outputs\x18\x06 \x03(\x0b\x32\x44.nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor\x12\x13\n\x0bsequence_id\x18\x07 \x01(\x04\x1a\xbb\x02\n\x10InferInputTensor\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08\x64\x61tatype\x18\x02 \x01(\t\x12\r\n\x05shape\x18\x03 \x03(\x03\x12^\n\nparameters\x18\x04 \x03(\x0b\x32J.nvidia.inferenceserver.ModelInferRequest.InferInputTensor.ParametersEntry\x12=\n\x08\x63ontents\x18\x05 \x01(\x0b\x32+.nvidia.inferenceserver.InferTensorContents\x1aY\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x35\n\x05value\x18\x02 \x01(\x0b\x32&.nvidia.inferenceserver.InferParameter:\x02\x38\x01\x1a\x84\x02\n\x1aInferRequestedOutputTensor\x12\x0c\n\x04name\x18\x01 \x01(\t\x12h\n\nparameters\x18\x02 \x03(\x0b\x32T.nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry\x12\x13\n\x0b\x64\x61ta_format\x18\x03 \x01(\t\x1aY\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x35\n\x05value\x18\x02 \x01(\x0b\x32&.nvidia.inferenceserver.InferParameter:\x02\x38\x01\x1aY\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x35\n\x05value\x18\x02 \x01(\x0b\x32&.nvidia.inferenceserver.InferParameter:\x02\x38\x01\"\xc2\x04\n\x12ModelInferResponse\x12\x12\n\nmodel_name\x18\x01 \x01(\t\x12\x15\n\rmodel_version\x18\x02 \x01(\x03\x12\n\n\x02id\x18\x03 \x01(\t\x12N\n\nparameters\x18\x04 \x03(\x0b\x32:.nvidia.inferenceserver.ModelInferResponse.ParametersEntry\x12:\n\x07request\x18\x05 \x01(\x0b\x32).nvidia.inferenceserver.ModelInferRequest\x12;\n\nstatistics\x18\x06 \x01(\x0b\x32\'.nvidia.inferenceserver.InferStatistics\x12M\n\x07outputs\x18\x07 \x03(\x0b\x32<.nvidia.inferenceserver.ModelInferResponse.InferOutputTensor\x1a\x81\x01\n\x11InferOutputTensor\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08\x64\x61tatype\x18\x02 \x01(\t\x12\r\n\x05shape\x18\x03 \x03(\x03\x12=\n\x08\x63ontents\x18\x04 \x01(\x0b\x32+.nvidia.inferenceserver.InferTensorContents\x1aY\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x35\n\x05value\x18\x02 \x01(\x0b\x32&.nvidia.inferenceserver.InferParameter:\x02\x38\x01\"3\n\x12ModelConfigRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\x03\"J\n\x13ModelConfigResponse\x12\x33\n\x06\x63onfig\x18\x01 \x01(\x0b\x32#.nvidia.inferenceserver.ModelConfig2\x82\x06\n\x14GRPCInferenceService\x12\x65\n\nServerLive\x12).nvidia.inferenceserver.ServerLiveRequest\x1a*.nvidia.inferenceserver.ServerLiveResponse\"\x00\x12h\n\x0bServerReady\x12*.nvidia.inferenceserver.ServerReadyRequest\x1a+.nvidia.inferenceserver.ServerReadyResponse\"\x00\x12\x65\n\nModelReady\x12).nvidia.inferenceserver.ModelReadyRequest\x1a*.nvidia.inferenceserver.ModelReadyResponse\"\x00\x12q\n\x0eServerMetadata\x12-.nvidia.inferenceserver.ServerMetadataRequest\x1a..nvidia.inferenceserver.ServerMetadataResponse\"\x00\x12n\n\rModelMetadata\x12,.nvidia.inferenceserver.ModelMetadataRequest\x1a-.nvidia.inferenceserver.ModelMetadataResponse\"\x00\x12\x65\n\nModelInfer\x12).nvidia.inferenceserver.ModelInferRequest\x1a*.nvidia.inferenceserver.ModelInferResponse\"\x00\x12h\n\x0bModelConfig\x12*.nvidia.inferenceserver.ModelConfigRequest\x1a+.nvidia.inferenceserver.ModelConfigResponse\"\x00\x62\x06proto3'
  ,
  dependencies=[model__config__pb2.DESCRIPTOR,])




_SERVERLIVEREQUEST = _descriptor.Descriptor(
  name='ServerLiveRequest',
  full_name='nvidia.inferenceserver.ServerLiveRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=69,
  serialized_end=88,
)


_SERVERLIVERESPONSE = _descriptor.Descriptor(
  name='ServerLiveResponse',
  full_name='nvidia.inferenceserver.ServerLiveResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='live', full_name='nvidia.inferenceserver.ServerLiveResponse.live', index=0,
      number=1, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=90,
  serialized_end=124,
)


_SERVERREADYREQUEST = _descriptor.Descriptor(
  name='ServerReadyRequest',
  full_name='nvidia.inferenceserver.ServerReadyRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=126,
  serialized_end=146,
)


_SERVERREADYRESPONSE = _descriptor.Descriptor(
  name='ServerReadyResponse',
  full_name='nvidia.inferenceserver.ServerReadyResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='ready', full_name='nvidia.inferenceserver.ServerReadyResponse.ready', index=0,
      number=1, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=148,
  serialized_end=184,
)


_MODELREADYREQUEST = _descriptor.Descriptor(
  name='ModelReadyRequest',
  full_name='nvidia.inferenceserver.ModelReadyRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelReadyRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='version', full_name='nvidia.inferenceserver.ModelReadyRequest.version', index=1,
      number=2, type=3, cpp_type=2, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=186,
  serialized_end=236,
)


_MODELREADYRESPONSE = _descriptor.Descriptor(
  name='ModelReadyResponse',
  full_name='nvidia.inferenceserver.ModelReadyResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='ready', full_name='nvidia.inferenceserver.ModelReadyResponse.ready', index=0,
      number=1, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=238,
  serialized_end=273,
)


_SERVERMETADATAREQUEST = _descriptor.Descriptor(
  name='ServerMetadataRequest',
  full_name='nvidia.inferenceserver.ServerMetadataRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=275,
  serialized_end=298,
)


_SERVERMETADATARESPONSE = _descriptor.Descriptor(
  name='ServerMetadataResponse',
  full_name='nvidia.inferenceserver.ServerMetadataResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ServerMetadataResponse.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='version', full_name='nvidia.inferenceserver.ServerMetadataResponse.version', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='extensions', full_name='nvidia.inferenceserver.ServerMetadataResponse.extensions', index=2,
      number=3, type=9, cpp_type=9, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=300,
  serialized_end=375,
)


_MODELMETADATAREQUEST = _descriptor.Descriptor(
  name='ModelMetadataRequest',
  full_name='nvidia.inferenceserver.ModelMetadataRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelMetadataRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='version', full_name='nvidia.inferenceserver.ModelMetadataRequest.version', index=1,
      number=2, type=3, cpp_type=2, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=377,
  serialized_end=430,
)


_MODELMETADATARESPONSE_TENSORMETADATA = _descriptor.Descriptor(
  name='TensorMetadata',
  full_name='nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='datatype', full_name='nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata.datatype', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='shape', full_name='nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata.shape', index=2,
      number=3, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=665,
  serialized_end=728,
)

_MODELMETADATARESPONSE = _descriptor.Descriptor(
  name='ModelMetadataResponse',
  full_name='nvidia.inferenceserver.ModelMetadataResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelMetadataResponse.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='versions', full_name='nvidia.inferenceserver.ModelMetadataResponse.versions', index=1,
      number=2, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='platform', full_name='nvidia.inferenceserver.ModelMetadataResponse.platform', index=2,
      number=3, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='inputs', full_name='nvidia.inferenceserver.ModelMetadataResponse.inputs', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='outputs', full_name='nvidia.inferenceserver.ModelMetadataResponse.outputs', index=4,
      number=5, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELMETADATARESPONSE_TENSORMETADATA, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=433,
  serialized_end=728,
)


_INFERPARAMETER_RESPONSEDETAIL = _descriptor.Descriptor(
  name='ResponseDetail',
  full_name='nvidia.inferenceserver.InferParameter.ResponseDetail',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='model', full_name='nvidia.inferenceserver.InferParameter.ResponseDetail.model', index=0,
      number=1, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='request', full_name='nvidia.inferenceserver.InferParameter.ResponseDetail.request', index=1,
      number=2, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='statistics', full_name='nvidia.inferenceserver.InferParameter.ResponseDetail.statistics', index=2,
      number=3, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='parameters', full_name='nvidia.inferenceserver.InferParameter.ResponseDetail.parameters', index=3,
      number=4, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=906,
  serialized_end=994,
)

_INFERPARAMETER = _descriptor.Descriptor(
  name='InferParameter',
  full_name='nvidia.inferenceserver.InferParameter',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='bool_param', full_name='nvidia.inferenceserver.InferParameter.bool_param', index=0,
      number=1, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='int64_param', full_name='nvidia.inferenceserver.InferParameter.int64_param', index=1,
      number=2, type=3, cpp_type=2, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='string_param', full_name='nvidia.inferenceserver.InferParameter.string_param', index=2,
      number=3, type=12, cpp_type=9, label=1,
      has_default_value=False, default_value=b"",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='response_detail_param', full_name='nvidia.inferenceserver.InferParameter.response_detail_param', index=3,
      number=4, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_INFERPARAMETER_RESPONSEDETAIL, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
    _descriptor.OneofDescriptor(
      name='parameter_choice', full_name='nvidia.inferenceserver.InferParameter.parameter_choice',
      index=0, containing_type=None, fields=[]),
  ],
  serialized_start=731,
  serialized_end=1014,
)


_STATISTICDURATION = _descriptor.Descriptor(
  name='StatisticDuration',
  full_name='nvidia.inferenceserver.StatisticDuration',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='count', full_name='nvidia.inferenceserver.StatisticDuration.count', index=0,
      number=1, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='ns', full_name='nvidia.inferenceserver.StatisticDuration.ns', index=1,
      number=2, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1016,
  serialized_end=1062,
)


_INFERSTATISTICS = _descriptor.Descriptor(
  name='InferStatistics',
  full_name='nvidia.inferenceserver.InferStatistics',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='success', full_name='nvidia.inferenceserver.InferStatistics.success', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='fail', full_name='nvidia.inferenceserver.InferStatistics.fail', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='queue', full_name='nvidia.inferenceserver.InferStatistics.queue', index=2,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='compute_input', full_name='nvidia.inferenceserver.InferStatistics.compute_input', index=3,
      number=4, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='compute_infer', full_name='nvidia.inferenceserver.InferStatistics.compute_infer', index=4,
      number=5, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='compute_output', full_name='nvidia.inferenceserver.InferStatistics.compute_output', index=5,
      number=6, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1065,
  serialized_end=1456,
)


_INFERTENSORCONTENTS = _descriptor.Descriptor(
  name='InferTensorContents',
  full_name='nvidia.inferenceserver.InferTensorContents',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='raw_contents', full_name='nvidia.inferenceserver.InferTensorContents.raw_contents', index=0,
      number=1, type=12, cpp_type=9, label=1,
      has_default_value=False, default_value=b"",
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='bool_contents', full_name='nvidia.inferenceserver.InferTensorContents.bool_contents', index=1,
      number=2, type=8, cpp_type=7, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='int_contents', full_name='nvidia.inferenceserver.InferTensorContents.int_contents', index=2,
      number=3, type=5, cpp_type=1, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='int64_contents', full_name='nvidia.inferenceserver.InferTensorContents.int64_contents', index=3,
      number=4, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='uint_contents', full_name='nvidia.inferenceserver.InferTensorContents.uint_contents', index=4,
      number=5, type=13, cpp_type=3, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='uint64_contents', full_name='nvidia.inferenceserver.InferTensorContents.uint64_contents', index=5,
      number=6, type=4, cpp_type=4, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='fp32_contents', full_name='nvidia.inferenceserver.InferTensorContents.fp32_contents', index=6,
      number=7, type=2, cpp_type=6, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='fp64_contents', full_name='nvidia.inferenceserver.InferTensorContents.fp64_contents', index=7,
      number=8, type=1, cpp_type=5, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='string_contents', full_name='nvidia.inferenceserver.InferTensorContents.string_contents', index=8,
      number=9, type=12, cpp_type=9, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1459,
  serialized_end=1690,
)


_MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY = _descriptor.Descriptor(
  name='ParametersEntry',
  full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.ParametersEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.ParametersEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.ParametersEntry.value', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=b'8\001',
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2259,
  serialized_end=2348,
)

_MODELINFERREQUEST_INFERINPUTTENSOR = _descriptor.Descriptor(
  name='InferInputTensor',
  full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='datatype', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.datatype', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='shape', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.shape', index=2,
      number=3, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='parameters', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.parameters', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='contents', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.contents', index=4,
      number=5, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2033,
  serialized_end=2348,
)

_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY = _descriptor.Descriptor(
  name='ParametersEntry',
  full_name='nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry.value', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=b'8\001',
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2259,
  serialized_end=2348,
)

_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR = _descriptor.Descriptor(
  name='InferRequestedOutputTensor',
  full_name='nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='parameters', full_name='nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.parameters', index=1,
      number=2, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='data_format', full_name='nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.data_format', index=2,
      number=3, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2351,
  serialized_end=2611,
)

_MODELINFERREQUEST_PARAMETERSENTRY = _descriptor.Descriptor(
  name='ParametersEntry',
  full_name='nvidia.inferenceserver.ModelInferRequest.ParametersEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelInferRequest.ParametersEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelInferRequest.ParametersEntry.value', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=b'8\001',
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2259,
  serialized_end=2348,
)

_MODELINFERREQUEST = _descriptor.Descriptor(
  name='ModelInferRequest',
  full_name='nvidia.inferenceserver.ModelInferRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='model_name', full_name='nvidia.inferenceserver.ModelInferRequest.model_name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='model_version', full_name='nvidia.inferenceserver.ModelInferRequest.model_version', index=1,
      number=2, type=3, cpp_type=2, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='id', full_name='nvidia.inferenceserver.ModelInferRequest.id', index=2,
      number=3, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='parameters', full_name='nvidia.inferenceserver.ModelInferRequest.parameters', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='inputs', full_name='nvidia.inferenceserver.ModelInferRequest.inputs', index=4,
      number=5, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='outputs', full_name='nvidia.inferenceserver.ModelInferRequest.outputs', index=5,
      number=6, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='sequence_id', full_name='nvidia.inferenceserver.ModelInferRequest.sequence_id', index=6,
      number=7, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELINFERREQUEST_INFERINPUTTENSOR, _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR, _MODELINFERREQUEST_PARAMETERSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1693,
  serialized_end=2702,
)


_MODELINFERRESPONSE_INFEROUTPUTTENSOR = _descriptor.Descriptor(
  name='InferOutputTensor',
  full_name='nvidia.inferenceserver.ModelInferResponse.InferOutputTensor',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelInferResponse.InferOutputTensor.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='datatype', full_name='nvidia.inferenceserver.ModelInferResponse.InferOutputTensor.datatype', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='shape', full_name='nvidia.inferenceserver.ModelInferResponse.InferOutputTensor.shape', index=2,
      number=3, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='contents', full_name='nvidia.inferenceserver.ModelInferResponse.InferOutputTensor.contents', index=3,
      number=4, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3063,
  serialized_end=3192,
)

_MODELINFERRESPONSE_PARAMETERSENTRY = _descriptor.Descriptor(
  name='ParametersEntry',
  full_name='nvidia.inferenceserver.ModelInferResponse.ParametersEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelInferResponse.ParametersEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelInferResponse.ParametersEntry.value', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=b'8\001',
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2259,
  serialized_end=2348,
)

_MODELINFERRESPONSE = _descriptor.Descriptor(
  name='ModelInferResponse',
  full_name='nvidia.inferenceserver.ModelInferResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='model_name', full_name='nvidia.inferenceserver.ModelInferResponse.model_name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='model_version', full_name='nvidia.inferenceserver.ModelInferResponse.model_version', index=1,
      number=2, type=3, cpp_type=2, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='id', full_name='nvidia.inferenceserver.ModelInferResponse.id', index=2,
      number=3, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='parameters', full_name='nvidia.inferenceserver.ModelInferResponse.parameters', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='request', full_name='nvidia.inferenceserver.ModelInferResponse.request', index=4,
      number=5, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='statistics', full_name='nvidia.inferenceserver.ModelInferResponse.statistics', index=5,
      number=6, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='outputs', full_name='nvidia.inferenceserver.ModelInferResponse.outputs', index=6,
      number=7, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELINFERRESPONSE_INFEROUTPUTTENSOR, _MODELINFERRESPONSE_PARAMETERSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2705,
  serialized_end=3283,
)


_MODELCONFIGREQUEST = _descriptor.Descriptor(
  name='ModelConfigRequest',
  full_name='nvidia.inferenceserver.ModelConfigRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelConfigRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='version', full_name='nvidia.inferenceserver.ModelConfigRequest.version', index=1,
      number=2, type=3, cpp_type=2, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3285,
  serialized_end=3336,
)


_MODELCONFIGRESPONSE = _descriptor.Descriptor(
  name='ModelConfigResponse',
  full_name='nvidia.inferenceserver.ModelConfigResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='config', full_name='nvidia.inferenceserver.ModelConfigResponse.config', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3338,
  serialized_end=3412,
)

_MODELMETADATARESPONSE_TENSORMETADATA.containing_type = _MODELMETADATARESPONSE
_MODELMETADATARESPONSE.fields_by_name['inputs'].message_type = _MODELMETADATARESPONSE_TENSORMETADATA
_MODELMETADATARESPONSE.fields_by_name['outputs'].message_type = _MODELMETADATARESPONSE_TENSORMETADATA
_INFERPARAMETER_RESPONSEDETAIL.containing_type = _INFERPARAMETER
_INFERPARAMETER.fields_by_name['response_detail_param'].message_type = _INFERPARAMETER_RESPONSEDETAIL
_INFERPARAMETER.oneofs_by_name['parameter_choice'].fields.append(
  _INFERPARAMETER.fields_by_name['bool_param'])
_INFERPARAMETER.fields_by_name['bool_param'].containing_oneof = _INFERPARAMETER.oneofs_by_name['parameter_choice']
_INFERPARAMETER.oneofs_by_name['parameter_choice'].fields.append(
  _INFERPARAMETER.fields_by_name['int64_param'])
_INFERPARAMETER.fields_by_name['int64_param'].containing_oneof = _INFERPARAMETER.oneofs_by_name['parameter_choice']
_INFERPARAMETER.oneofs_by_name['parameter_choice'].fields.append(
  _INFERPARAMETER.fields_by_name['string_param'])
_INFERPARAMETER.fields_by_name['string_param'].containing_oneof = _INFERPARAMETER.oneofs_by_name['parameter_choice']
_INFERPARAMETER.oneofs_by_name['parameter_choice'].fields.append(
  _INFERPARAMETER.fields_by_name['response_detail_param'])
_INFERPARAMETER.fields_by_name['response_detail_param'].containing_oneof = _INFERPARAMETER.oneofs_by_name['parameter_choice']
_INFERSTATISTICS.fields_by_name['success'].message_type = _STATISTICDURATION
_INFERSTATISTICS.fields_by_name['fail'].message_type = _STATISTICDURATION
_INFERSTATISTICS.fields_by_name['queue'].message_type = _STATISTICDURATION
_INFERSTATISTICS.fields_by_name['compute_input'].message_type = _STATISTICDURATION
_INFERSTATISTICS.fields_by_name['compute_infer'].message_type = _STATISTICDURATION
_INFERSTATISTICS.fields_by_name['compute_output'].message_type = _STATISTICDURATION
_MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY.fields_by_name['value'].message_type = _INFERPARAMETER
_MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY.containing_type = _MODELINFERREQUEST_INFERINPUTTENSOR
_MODELINFERREQUEST_INFERINPUTTENSOR.fields_by_name['parameters'].message_type = _MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY
_MODELINFERREQUEST_INFERINPUTTENSOR.fields_by_name['contents'].message_type = _INFERTENSORCONTENTS
_MODELINFERREQUEST_INFERINPUTTENSOR.containing_type = _MODELINFERREQUEST
_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY.fields_by_name['value'].message_type = _INFERPARAMETER
_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY.containing_type = _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR
_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR.fields_by_name['parameters'].message_type = _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY
_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR.containing_type = _MODELINFERREQUEST
_MODELINFERREQUEST_PARAMETERSENTRY.fields_by_name['value'].message_type = _INFERPARAMETER
_MODELINFERREQUEST_PARAMETERSENTRY.containing_type = _MODELINFERREQUEST
_MODELINFERREQUEST.fields_by_name['parameters'].message_type = _MODELINFERREQUEST_PARAMETERSENTRY
_MODELINFERREQUEST.fields_by_name['inputs'].message_type = _MODELINFERREQUEST_INFERINPUTTENSOR
_MODELINFERREQUEST.fields_by_name['outputs'].message_type = _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR
_MODELINFERRESPONSE_INFEROUTPUTTENSOR.fields_by_name['contents'].message_type = _INFERTENSORCONTENTS
_MODELINFERRESPONSE_INFEROUTPUTTENSOR.containing_type = _MODELINFERRESPONSE
_MODELINFERRESPONSE_PARAMETERSENTRY.fields_by_name['value'].message_type = _INFERPARAMETER
_MODELINFERRESPONSE_PARAMETERSENTRY.containing_type = _MODELINFERRESPONSE
_MODELINFERRESPONSE.fields_by_name['parameters'].message_type = _MODELINFERRESPONSE_PARAMETERSENTRY
_MODELINFERRESPONSE.fields_by_name['request'].message_type = _MODELINFERREQUEST
_MODELINFERRESPONSE.fields_by_name['statistics'].message_type = _INFERSTATISTICS
_MODELINFERRESPONSE.fields_by_name['outputs'].message_type = _MODELINFERRESPONSE_INFEROUTPUTTENSOR
_MODELCONFIGRESPONSE.fields_by_name['config'].message_type = model__config__pb2._MODELCONFIG
DESCRIPTOR.message_types_by_name['ServerLiveRequest'] = _SERVERLIVEREQUEST
DESCRIPTOR.message_types_by_name['ServerLiveResponse'] = _SERVERLIVERESPONSE
DESCRIPTOR.message_types_by_name['ServerReadyRequest'] = _SERVERREADYREQUEST
DESCRIPTOR.message_types_by_name['ServerReadyResponse'] = _SERVERREADYRESPONSE
DESCRIPTOR.message_types_by_name['ModelReadyRequest'] = _MODELREADYREQUEST
DESCRIPTOR.message_types_by_name['ModelReadyResponse'] = _MODELREADYRESPONSE
DESCRIPTOR.message_types_by_name['ServerMetadataRequest'] = _SERVERMETADATAREQUEST
DESCRIPTOR.message_types_by_name['ServerMetadataResponse'] = _SERVERMETADATARESPONSE
DESCRIPTOR.message_types_by_name['ModelMetadataRequest'] = _MODELMETADATAREQUEST
DESCRIPTOR.message_types_by_name['ModelMetadataResponse'] = _MODELMETADATARESPONSE
DESCRIPTOR.message_types_by_name['InferParameter'] = _INFERPARAMETER
DESCRIPTOR.message_types_by_name['StatisticDuration'] = _STATISTICDURATION
DESCRIPTOR.message_types_by_name['InferStatistics'] = _INFERSTATISTICS
DESCRIPTOR.message_types_by_name['InferTensorContents'] = _INFERTENSORCONTENTS
DESCRIPTOR.message_types_by_name['ModelInferRequest'] = _MODELINFERREQUEST
DESCRIPTOR.message_types_by_name['ModelInferResponse'] = _MODELINFERRESPONSE
DESCRIPTOR.message_types_by_name['ModelConfigRequest'] = _MODELCONFIGREQUEST
DESCRIPTOR.message_types_by_name['ModelConfigResponse'] = _MODELCONFIGRESPONSE
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

ServerLiveRequest = _reflection.GeneratedProtocolMessageType('ServerLiveRequest', (_message.Message,), {
  'DESCRIPTOR' : _SERVERLIVEREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerLiveRequest)
  })
_sym_db.RegisterMessage(ServerLiveRequest)

ServerLiveResponse = _reflection.GeneratedProtocolMessageType('ServerLiveResponse', (_message.Message,), {
  'DESCRIPTOR' : _SERVERLIVERESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerLiveResponse)
  })
_sym_db.RegisterMessage(ServerLiveResponse)

ServerReadyRequest = _reflection.GeneratedProtocolMessageType('ServerReadyRequest', (_message.Message,), {
  'DESCRIPTOR' : _SERVERREADYREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerReadyRequest)
  })
_sym_db.RegisterMessage(ServerReadyRequest)

ServerReadyResponse = _reflection.GeneratedProtocolMessageType('ServerReadyResponse', (_message.Message,), {
  'DESCRIPTOR' : _SERVERREADYRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerReadyResponse)
  })
_sym_db.RegisterMessage(ServerReadyResponse)

ModelReadyRequest = _reflection.GeneratedProtocolMessageType('ModelReadyRequest', (_message.Message,), {
  'DESCRIPTOR' : _MODELREADYREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelReadyRequest)
  })
_sym_db.RegisterMessage(ModelReadyRequest)

ModelReadyResponse = _reflection.GeneratedProtocolMessageType('ModelReadyResponse', (_message.Message,), {
  'DESCRIPTOR' : _MODELREADYRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelReadyResponse)
  })
_sym_db.RegisterMessage(ModelReadyResponse)

ServerMetadataRequest = _reflection.GeneratedProtocolMessageType('ServerMetadataRequest', (_message.Message,), {
  'DESCRIPTOR' : _SERVERMETADATAREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerMetadataRequest)
  })
_sym_db.RegisterMessage(ServerMetadataRequest)

ServerMetadataResponse = _reflection.GeneratedProtocolMessageType('ServerMetadataResponse', (_message.Message,), {
  'DESCRIPTOR' : _SERVERMETADATARESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerMetadataResponse)
  })
_sym_db.RegisterMessage(ServerMetadataResponse)

ModelMetadataRequest = _reflection.GeneratedProtocolMessageType('ModelMetadataRequest', (_message.Message,), {
  'DESCRIPTOR' : _MODELMETADATAREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelMetadataRequest)
  })
_sym_db.RegisterMessage(ModelMetadataRequest)

ModelMetadataResponse = _reflection.GeneratedProtocolMessageType('ModelMetadataResponse', (_message.Message,), {

  'TensorMetadata' : _reflection.GeneratedProtocolMessageType('TensorMetadata', (_message.Message,), {
    'DESCRIPTOR' : _MODELMETADATARESPONSE_TENSORMETADATA,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata)
    })
  ,
  'DESCRIPTOR' : _MODELMETADATARESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelMetadataResponse)
  })
_sym_db.RegisterMessage(ModelMetadataResponse)
_sym_db.RegisterMessage(ModelMetadataResponse.TensorMetadata)

InferParameter = _reflection.GeneratedProtocolMessageType('InferParameter', (_message.Message,), {

  'ResponseDetail' : _reflection.GeneratedProtocolMessageType('ResponseDetail', (_message.Message,), {
    'DESCRIPTOR' : _INFERPARAMETER_RESPONSEDETAIL,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferParameter.ResponseDetail)
    })
  ,
  'DESCRIPTOR' : _INFERPARAMETER,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferParameter)
  })
_sym_db.RegisterMessage(InferParameter)
_sym_db.RegisterMessage(InferParameter.ResponseDetail)

StatisticDuration = _reflection.GeneratedProtocolMessageType('StatisticDuration', (_message.Message,), {
  'DESCRIPTOR' : _STATISTICDURATION,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.StatisticDuration)
  })
_sym_db.RegisterMessage(StatisticDuration)

InferStatistics = _reflection.GeneratedProtocolMessageType('InferStatistics', (_message.Message,), {
  'DESCRIPTOR' : _INFERSTATISTICS,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferStatistics)
  })
_sym_db.RegisterMessage(InferStatistics)

InferTensorContents = _reflection.GeneratedProtocolMessageType('InferTensorContents', (_message.Message,), {
  'DESCRIPTOR' : _INFERTENSORCONTENTS,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferTensorContents)
  })
_sym_db.RegisterMessage(InferTensorContents)

ModelInferRequest = _reflection.GeneratedProtocolMessageType('ModelInferRequest', (_message.Message,), {

  'InferInputTensor' : _reflection.GeneratedProtocolMessageType('InferInputTensor', (_message.Message,), {

    'ParametersEntry' : _reflection.GeneratedProtocolMessageType('ParametersEntry', (_message.Message,), {
      'DESCRIPTOR' : _MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY,
      '__module__' : 'grpc_service_v2_pb2'
      # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferRequest.InferInputTensor.ParametersEntry)
      })
    ,
    'DESCRIPTOR' : _MODELINFERREQUEST_INFERINPUTTENSOR,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferRequest.InferInputTensor)
    })
  ,

  'InferRequestedOutputTensor' : _reflection.GeneratedProtocolMessageType('InferRequestedOutputTensor', (_message.Message,), {

    'ParametersEntry' : _reflection.GeneratedProtocolMessageType('ParametersEntry', (_message.Message,), {
      'DESCRIPTOR' : _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY,
      '__module__' : 'grpc_service_v2_pb2'
      # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry)
      })
    ,
    'DESCRIPTOR' : _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor)
    })
  ,

  'ParametersEntry' : _reflection.GeneratedProtocolMessageType('ParametersEntry', (_message.Message,), {
    'DESCRIPTOR' : _MODELINFERREQUEST_PARAMETERSENTRY,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferRequest.ParametersEntry)
    })
  ,
  'DESCRIPTOR' : _MODELINFERREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferRequest)
  })
_sym_db.RegisterMessage(ModelInferRequest)
_sym_db.RegisterMessage(ModelInferRequest.InferInputTensor)
_sym_db.RegisterMessage(ModelInferRequest.InferInputTensor.ParametersEntry)
_sym_db.RegisterMessage(ModelInferRequest.InferRequestedOutputTensor)
_sym_db.RegisterMessage(ModelInferRequest.InferRequestedOutputTensor.ParametersEntry)
_sym_db.RegisterMessage(ModelInferRequest.ParametersEntry)

ModelInferResponse = _reflection.GeneratedProtocolMessageType('ModelInferResponse', (_message.Message,), {

  'InferOutputTensor' : _reflection.GeneratedProtocolMessageType('InferOutputTensor', (_message.Message,), {
    'DESCRIPTOR' : _MODELINFERRESPONSE_INFEROUTPUTTENSOR,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferResponse.InferOutputTensor)
    })
  ,

  'ParametersEntry' : _reflection.GeneratedProtocolMessageType('ParametersEntry', (_message.Message,), {
    'DESCRIPTOR' : _MODELINFERRESPONSE_PARAMETERSENTRY,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferResponse.ParametersEntry)
    })
  ,
  'DESCRIPTOR' : _MODELINFERRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferResponse)
  })
_sym_db.RegisterMessage(ModelInferResponse)
_sym_db.RegisterMessage(ModelInferResponse.InferOutputTensor)
_sym_db.RegisterMessage(ModelInferResponse.ParametersEntry)

ModelConfigRequest = _reflection.GeneratedProtocolMessageType('ModelConfigRequest', (_message.Message,), {
  'DESCRIPTOR' : _MODELCONFIGREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfigRequest)
  })
_sym_db.RegisterMessage(ModelConfigRequest)

ModelConfigResponse = _reflection.GeneratedProtocolMessageType('ModelConfigResponse', (_message.Message,), {
  'DESCRIPTOR' : _MODELCONFIGRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfigResponse)
  })
_sym_db.RegisterMessage(ModelConfigResponse)


_MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY._options = None
_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY._options = None
_MODELINFERREQUEST_PARAMETERSENTRY._options = None
_MODELINFERRESPONSE_PARAMETERSENTRY._options = None

_GRPCINFERENCESERVICE = _descriptor.ServiceDescriptor(
  name='GRPCInferenceService',
  full_name='nvidia.inferenceserver.GRPCInferenceService',
  file=DESCRIPTOR,
  index=0,
  serialized_options=None,
  serialized_start=3415,
  serialized_end=4185,
  methods=[
  _descriptor.MethodDescriptor(
    name='ServerLive',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ServerLive',
    index=0,
    containing_service=None,
    input_type=_SERVERLIVEREQUEST,
    output_type=_SERVERLIVERESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ServerReady',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ServerReady',
    index=1,
    containing_service=None,
    input_type=_SERVERREADYREQUEST,
    output_type=_SERVERREADYRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ModelReady',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ModelReady',
    index=2,
    containing_service=None,
    input_type=_MODELREADYREQUEST,
    output_type=_MODELREADYRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ServerMetadata',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ServerMetadata',
    index=3,
    containing_service=None,
    input_type=_SERVERMETADATAREQUEST,
    output_type=_SERVERMETADATARESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ModelMetadata',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ModelMetadata',
    index=4,
    containing_service=None,
    input_type=_MODELMETADATAREQUEST,
    output_type=_MODELMETADATARESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ModelInfer',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ModelInfer',
    index=5,
    containing_service=None,
    input_type=_MODELINFERREQUEST,
    output_type=_MODELINFERRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ModelConfig',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ModelConfig',
    index=6,
    containing_service=None,
    input_type=_MODELCONFIGREQUEST,
    output_type=_MODELCONFIGRESPONSE,
    serialized_options=None,
  ),
])
_sym_db.RegisterServiceDescriptor(_GRPCINFERENCESERVICE)

DESCRIPTOR.services_by_name['GRPCInferenceService'] = _GRPCINFERENCESERVICE

# @@protoc_insertion_point(module_scope)
