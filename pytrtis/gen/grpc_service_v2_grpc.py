# Generated by the Protocol Buffers compiler. DO NOT EDIT!
# source: grpc_service_v2.proto
# plugin: grpclib.plugin.main
import abc
import typing

import grpclib.const
import grpclib.client
if typing.TYPE_CHECKING:
    import grpclib.server

from pytrtis.gen import model_config_pb2
from pytrtis.gen import grpc_service_v2_pb2


class GRPCInferenceServiceBase(abc.ABC):

    @abc.abstractmethod
    async def ServerLive(self, stream: 'grpclib.server.Stream[grpc_service_v2_pb2.ServerLiveRequest, grpc_service_v2_pb2.ServerLiveResponse]') -> None:
        pass

    @abc.abstractmethod
    async def ServerReady(self, stream: 'grpclib.server.Stream[grpc_service_v2_pb2.ServerReadyRequest, grpc_service_v2_pb2.ServerReadyResponse]') -> None:
        pass

    @abc.abstractmethod
    async def ModelReady(self, stream: 'grpclib.server.Stream[grpc_service_v2_pb2.ModelReadyRequest, grpc_service_v2_pb2.ModelReadyResponse]') -> None:
        pass

    @abc.abstractmethod
    async def ServerMetadata(self, stream: 'grpclib.server.Stream[grpc_service_v2_pb2.ServerMetadataRequest, grpc_service_v2_pb2.ServerMetadataResponse]') -> None:
        pass

    @abc.abstractmethod
    async def ModelMetadata(self, stream: 'grpclib.server.Stream[grpc_service_v2_pb2.ModelMetadataRequest, grpc_service_v2_pb2.ModelMetadataResponse]') -> None:
        pass

    @abc.abstractmethod
    async def ModelInfer(self, stream: 'grpclib.server.Stream[grpc_service_v2_pb2.ModelInferRequest, grpc_service_v2_pb2.ModelInferResponse]') -> None:
        pass

    @abc.abstractmethod
    async def ModelConfig(self, stream: 'grpclib.server.Stream[grpc_service_v2_pb2.ModelConfigRequest, grpc_service_v2_pb2.ModelConfigResponse]') -> None:
        pass

    def __mapping__(self) -> typing.Dict[str, grpclib.const.Handler]:
        return {
            '/nvidia.inferenceserver.GRPCInferenceService/ServerLive': grpclib.const.Handler(
                self.ServerLive,
                grpclib.const.Cardinality.UNARY_UNARY,
                grpc_service_v2_pb2.ServerLiveRequest,
                grpc_service_v2_pb2.ServerLiveResponse,
            ),
            '/nvidia.inferenceserver.GRPCInferenceService/ServerReady': grpclib.const.Handler(
                self.ServerReady,
                grpclib.const.Cardinality.UNARY_UNARY,
                grpc_service_v2_pb2.ServerReadyRequest,
                grpc_service_v2_pb2.ServerReadyResponse,
            ),
            '/nvidia.inferenceserver.GRPCInferenceService/ModelReady': grpclib.const.Handler(
                self.ModelReady,
                grpclib.const.Cardinality.UNARY_UNARY,
                grpc_service_v2_pb2.ModelReadyRequest,
                grpc_service_v2_pb2.ModelReadyResponse,
            ),
            '/nvidia.inferenceserver.GRPCInferenceService/ServerMetadata': grpclib.const.Handler(
                self.ServerMetadata,
                grpclib.const.Cardinality.UNARY_UNARY,
                grpc_service_v2_pb2.ServerMetadataRequest,
                grpc_service_v2_pb2.ServerMetadataResponse,
            ),
            '/nvidia.inferenceserver.GRPCInferenceService/ModelMetadata': grpclib.const.Handler(
                self.ModelMetadata,
                grpclib.const.Cardinality.UNARY_UNARY,
                grpc_service_v2_pb2.ModelMetadataRequest,
                grpc_service_v2_pb2.ModelMetadataResponse,
            ),
            '/nvidia.inferenceserver.GRPCInferenceService/ModelInfer': grpclib.const.Handler(
                self.ModelInfer,
                grpclib.const.Cardinality.UNARY_UNARY,
                grpc_service_v2_pb2.ModelInferRequest,
                grpc_service_v2_pb2.ModelInferResponse,
            ),
            '/nvidia.inferenceserver.GRPCInferenceService/ModelConfig': grpclib.const.Handler(
                self.ModelConfig,
                grpclib.const.Cardinality.UNARY_UNARY,
                grpc_service_v2_pb2.ModelConfigRequest,
                grpc_service_v2_pb2.ModelConfigResponse,
            ),
        }


class GRPCInferenceServiceStub:

    def __init__(self, channel: grpclib.client.Channel) -> None:
        self.ServerLive = grpclib.client.UnaryUnaryMethod(
            channel,
            '/nvidia.inferenceserver.GRPCInferenceService/ServerLive',
            grpc_service_v2_pb2.ServerLiveRequest,
            grpc_service_v2_pb2.ServerLiveResponse,
        )
        self.ServerReady = grpclib.client.UnaryUnaryMethod(
            channel,
            '/nvidia.inferenceserver.GRPCInferenceService/ServerReady',
            grpc_service_v2_pb2.ServerReadyRequest,
            grpc_service_v2_pb2.ServerReadyResponse,
        )
        self.ModelReady = grpclib.client.UnaryUnaryMethod(
            channel,
            '/nvidia.inferenceserver.GRPCInferenceService/ModelReady',
            grpc_service_v2_pb2.ModelReadyRequest,
            grpc_service_v2_pb2.ModelReadyResponse,
        )
        self.ServerMetadata = grpclib.client.UnaryUnaryMethod(
            channel,
            '/nvidia.inferenceserver.GRPCInferenceService/ServerMetadata',
            grpc_service_v2_pb2.ServerMetadataRequest,
            grpc_service_v2_pb2.ServerMetadataResponse,
        )
        self.ModelMetadata = grpclib.client.UnaryUnaryMethod(
            channel,
            '/nvidia.inferenceserver.GRPCInferenceService/ModelMetadata',
            grpc_service_v2_pb2.ModelMetadataRequest,
            grpc_service_v2_pb2.ModelMetadataResponse,
        )
        self.ModelInfer = grpclib.client.UnaryUnaryMethod(
            channel,
            '/nvidia.inferenceserver.GRPCInferenceService/ModelInfer',
            grpc_service_v2_pb2.ModelInferRequest,
            grpc_service_v2_pb2.ModelInferResponse,
        )
        self.ModelConfig = grpclib.client.UnaryUnaryMethod(
            channel,
            '/nvidia.inferenceserver.GRPCInferenceService/ModelConfig',
            grpc_service_v2_pb2.ModelConfigRequest,
            grpc_service_v2_pb2.ModelConfigResponse,
        )
